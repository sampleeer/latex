\documentclass[a4paper, twoside, 12pt]{article}
\usepackage{amssymb}
% allows for temporary adjustment of side margins
\usepackage{chngpage}
% provides filler text
\usepackage{wrapfig}
\usepackage{lipsum}
% just makes the table prettier (see \toprule, \bottomrule, etc. commands below)
\usepackage{booktabs}
% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[russian]{babel}

% 257 -> 276 

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
% колонтитул 
\usepackage{fancyhdr} 
\fancyhf{}
\fancyhead[LE]{\thesection}
\fancyhead[RO]{257}
%\fancyhead[R]{Тут будет текст на правой стороне}
\pagestyle{fancy}
\DeclareMathOperator{\Prob}{P}

\begin{document}
\noindent  левых элементов. Обычно эти матрицы имеют так называемую \textit{ленточную структуру}. Более точно, матрицу A называют (2q + 1)-диагональной или имеющей \textit{ленточную структуру}, если $a_{i j} = 0$ при |i \text{-} j| > q.
Число 2q+1 называют \textit{шириной ленты}. Оказывается, что при решении системы уравнений с ленточной матрицей методом Гаусса число арифметических операций и требуемый объем памяти ЭВМ могут быть существенно сокращены.
\\ \\ 
\chapter{\textbf{Задача 1.}} Исследовать характеристики метода Гаусса и метода решения системы с помощью разложения ленточной матрицы A на произведение левой и правой треугольных матриц. Показать, что для нахождения решения требуется \mathcal{O}(\textit{mq}2) арифметических операций (при $m, q \to \infty$). Найти главный член числа операций при условии $1 \ll q \ll m$.
\\ \\ 
\chapter{\textbf{Задача 2.}}
Оценить объем загружаемой памяти \textbf{ЭВМ} в методе Гаусса для ленточных матриц.
\\ \\ 
\begin{adjustwidth}{1cm}{0cm}
    \small
   При вычислениях без помощи ЭВМ велика вероятность случайных погрешностей. Для устранения таких погрешностей иногда вводят \textit{контрольный столбец системы} $a_{m+2} = (a_{1}, \; _{m+2}, ..., a_{m}, \; _{ m+2})^T$, состоящий из контрольных эле- ментов уравнений системы
   \[ a_{i}, _{m+2} =  \sum_{j=1}^{k+1} a_{ij}.\]
   При преобразовании уравнений над контрольными элементами производятся те же операции, что и над свободными членами уравнений. В результате этого контрольный элемент каждого нового уравнения должен равняться сумме коэффициентов этого уравнения. Большое расхождение между ними указывает на погрешности в вычислениях или на неустойчивость алгоритма вычислений по отношению к вычислительной погрешности.
   \\ 
   К примеру, в случае приведения системы уравнений $A \textbf{x} = \textbf{b}$ к виду $D \textbf{x} = \textbf{d}$ \\
   с помощью формул (4) контрольный элемент $d_{i}, \; _{m+2}$ каждого из уравнений системы $D \textbf{x} = \textbf{d}$ вычисляется по тем же формулам (4). После вычисления всех элементов $d_{ij}$ при фиксированном $i$ контроль осуществляется проверкой равенства
    \[\sum_{j=i}^{m+1} d_{ij} = d_{i}, \; _{m+2}.\]
    \\ 
    Обратный ход метода Гаусса также сопровождается вычислением контрольных элементов строк системы.
\end{adjustwidth} 
\\ 
Чтобы избежать катастрофического влияния вычислительной погрешности, применяют метод Гаусса с выбором главного элемента. Его отличие
от описанной выше схемы метода Гаусса состоит в следующем. Пусть по
ходу исключения неизвестных получена система уравнений
\[ x_{i} + \sum_{j=i + 1}^{m} a_{ij}^ix_{j} = a_{i}^i, \; _{m+1},\; \; i = 1, ..., k,  \]

\[ \sum_{j=k + 1}^{m} a_{ij}^kx_{j} = a_{i}^k, \; _{m+1},\; \; i = k+1, ..., m,  \]
\\
Найдем $l$ такое, что
$\abs{a_{k+1}^k _{,l}}$ = $\max{ \abs{a_{k+1}^k _{,j}}}$ и переобозначим
$x_{k+1} = x_{l}$ и 
$x_{l} = x_{k + 1};$ 
далее произведем исключение неизвестной $x_{k+1}$ из всех урав- нений, начиная с $(k + 2)$-го. Такое переобозначение приводит к изменению порядка исключения неизвестных и во многих случаях существенно уменьшает чувствительность решения к погрешностям округления при вычислениях.
Часто требуется решить несколько систем уравнений  $A \textbf{x} = \textbf{b}_{q}, q = 1, ..., p,$  с одной и той же матрицей $A$. Удобно поступить следующим образом: введя обозначения
\[ \textbf{b}_{q} = (a_{1}, _{m+q}, ...,  a_{m}, _{m+q})^T, \; q = 1, ..., p.\]
произведем вычисления по формулам (4), причем элементы dik вычислим при i < k  m + p. В результате будут получены p систем уравнений с треугольной матрицей, соответствующих исходной задаче
\[D \textbf{x} = \textbf{d}_{q}, 
\textbf{d}_{q} = (d{1}, _{m+q}, ...,  d_{m}, _{m+q})^T, q = 1, ..., p.\]
\\
Решаем эти системы каждую в отдельности. Оказывается, что общее чис- ло арифметических действий при решении $p$ систем уравнений таким способом \\ $N \sim 2m^3/3 + 2pm^2$

\begin{adjustwidth}{1cm}{0cm}

\begin{small}
Описанный выше прием иногда используется для того, чтобы без существен- ных дополнительных затрат получить суждение о погрешности решения, яв- ляющейся следствием погрешностей округления при вычислениях. Задаются вектором $z$ с компонентами, имеющими по возможности тот же порядок и знак, что и компоненты искомого решения; часто из-за отсутствия достаточной информации берут $z = (1, ..., 1)^T.$ 
Вычисляется вектор $\textbf{c} = A\textbf{z}$, и наряду с исходной системой уравнений решается система $\textbf{A}z = \textbf{c}$.
\\ 
Пусть $\textbf{x}'$ и $\textbf{z'}$ — реально получаемые решения этих систем. Суждение о погреш- ности $\textbf{x}' - \textbf{x}$ искомого решения можно получить, основываясь на гипотезе: относительные погрешности при решении методом исключения систем с одной и той же матрицей и различными правыми частями, которыми являются соответственно величины $ \Vert \textbf{x - x}' \Vert \ /  \Vert \textbf{x}' \Vert \ $ и $ \Vert \textbf{z - z}' \Vert \ /  \Vert \textbf{z}' \Vert \ $, отличаются не в очень большое число раз.\\

Другой прием для получения суждения о реальной величине погрешности, воз- никающей за счет округлений при вычислениях, состоит в \textit{изменении масштабов}, меняющем картину накопления вычислительной погрешности. Наряду с
исходной системой тем же методом решается система \\

\begin{center}
 $(\alpha A)\textbf{x}' = \beta \textbf{b} $, где $\alpha$ и $\beta - $ числа.
\end{center}
 При $\alpha$ и $\beta$, не являющихся целыми степенями двойки, сравнение векторов $\textbf{x}$ и
 $\alpha \beta^{-1} \textbf{x}'$ дает представление о величине вычислительной погрешности. Например,можно взять $\alpha$ = $\sqrt{2}$, $\beta$= $\sqrt{3}$.
\end{small}

\end{adjustwidth} 
\\ 
\indent Изучение многих задач приводит к необходимости решения систем ли- нейных уравнений с симметричной положительно определенной матрицей. Такие системы возникают, например, при решении дифференциальных уравнений методом конечных элементов или же конечно-разностными ме- тодами. В этих случаях матрица системы имеет также и ленточную структуру. \\ 
\indent Для решения таких систем, а также систем уравнений более обще- го вида с эрмитовой не обязательно положительно определенной матрицей применяется \textit{метод квадратного корня (метод Холецкого)}. Матрица A представляется в виде
\[A = S^*DS,\]
где $S$—правая треугольная матрица, $S^*$ - сопряженная с ней, т.е.
\[ S = 
\left(
\begin{array}{ccc}
s_{11} & s_{12}& ... \\
0 & s_{22} & ... \\
...& ... & ...
\end{array}
\right) , 
\]

\noindent причем все $s_{ii} > 0$, $D$ — диагональная матрица с элементами $d_{ii}$, равными
$+1$ или $−1$. Матричное равенство (6) образует систему уравнений 
\[ a_{ij} = \sum_{k=1}^{i} \Bar{s_{ki}}s_{kj}d_{kk} = \Bar{s_{1i}}s_{1j}d_{11}+...+\Bar{s_{ii}}s_{ij}d_{ii} \; \; \; \text{при} \; \; \; i \leq j.\] 
Аналогичные уравнения при $i > j$ отброшены, так как уравнения, соот-
ветствующие парам $(i, j)$ и $(j, i)$, эквивалентны. Отсюда получаем рекуррентные формулы для определения элементов $d_{ii}$ и $s_{ij}:$

\[ d_{ii} = sign \Biggl( a_{ii} \sum_{k=1}^{i - 1} \abs{s_ki}^2d_{kk} \Biggl), \; \; \; 
s_{ii} = \sqrt{\abs{a_{ii} - \sum_{k=1}^{i - 1} \abs{s_ki}^2d_{kk}
}} , \] 
\\
\[ s_{ij} = \frac{a_{ij} - \sum\limits_{k=1}^{i - 1}{\Bar{s_{ki}}s_{kj}d_{kk}}}
{{s_{ii}d_{ii}}} \; \; \;  \text{при} \; i < j. \]

\\
\indent Матрица S является правой треугольной, и, таким образом, после по- лучения представления (6) решение исходной системы также сводится к последовательному решению двух систем с треугольными матрицами. Заметим, что в случае $A>0$ все $d_{ii}$ = 1 и $A=S^*S$.
\\ 
\chapter{\textbf{Задача 3.}}
Оценить число арифметических операций и загрузку памяти ЭВМ (при условии $a_{ij} = a_{ji}$ объем памяти, требуемый для запоминания матрицы $A$, уменьшается) при решении системы с вещественной положи- тельно определеннной матрицей $A$ методом квадратного корня.
\\
\indent Многие пакеты прикладных программ для решения краевых задач ма- тематической физики методом конечных элементов организованы по сле- дующей схеме. После формирования матрицы системы A путем переста- новки строк и столбцов (одновременно переставляются $i$-я и $j$-я строки и $i$-й и $j$-й столбцы) система преобразуется к виду с наименьшей шириной ленты. Далее применяется метод квадратного корня. При этом с целью уменьшения объема вычислений при решении системы $A \textbf{x} = \textbf{b}$ с другими правыми частями матрица $S$ запоминается.
\\ \\ 
\textit{Замечание. }Часто этот метод уступает по эффективности итерационным методам.
\\ 
\\
\chapter{\textbf{Задача 4.}}
Оценить число арифметических операций и объем требуемой памяти метода квадратного корня в случае матриц ленточной структуры.
\\ \\ 
\indent
Если есть подозрение, что реально полученное решение $x^1$ сильно искажено вычислительной погрешностью, то можно поступить следующим образом. Определим вектор $\textbf{b}^1 = \textbf{b} - A\textbf{x}^1.$ Погрешность $\textbf{r}^1 = \textbf{x}-  \textbf{x}^1$ удо- влетворяет системе уравнений
\[ A\textbf{r}^1 = A\textbf{x} - A\textbf{x}^1 = \textbf{b}^1.\]

Решая эту систему в условиях реальных округлений, получаем прибли- жение $\textbf{r}^{(1)}$ к $\textbf{r}^{1}$. Полагаем $\textbf{x}^{2} = \textbf{x}^{1} + \textbf{r}^{(1)}$. Если точность нового прибли- жения представляется неудовлетворительной, то повторяем эту операцию. При решении системы $(7)$ над компонентами правой части производятся те же линейные операции, что и над компонентами правой части при решении системы $(1)$. Поэтому при вычислениях на ЭВМ с плавающей запятой естественно ожидать, что относительные погрешности решений этих систем будут одного порядка. Поскольку погрешности округлений обычно малы, то $ \Vert \textbf{b}^1 \Vert \ \ll  \Vert \textbf{b} \Vert \ $ тогда $ \Vert \textbf{r}^1 \Vert \ \ll  \Vert \textbf{x}^1 \Vert \ $, и, как правило, решение $(7)$ определится с существенно меньшей абсолютной погрешностью, чем решение системы $(1)$. Таким образом, применение описанного прие- ма приводит к повышению точности приближенного решения. \\ 
\indent Особенно удобно применять этот прием, когда по ходу вычислений в памяти ЭВМ сохраняются матрицы \textit{B} и \textit{D}. Тогда для каждого уточнения требуется найти вектор $\textbf{b}^k = \textbf{b} - A\textbf{x}^k$ и решить две системы с треугольными матрицами. Это потребует всего $N_{1} \sim 4m^2 $арифметических операций, что составит малую долю от числа операций $N_{0} \sim 2m^3/3$, требующихся для представления матрицы A в виде $A = BD$.
\\
\begin{adjustwidth}{1cm}{0cm}

\begin{small}
Идея описанного приема последовательного уточнения приближений к реше- нию часто реализуется в такой форме. Пусть матрица $B$ близка в каком-то
 смысле к матрице A, но решение системы $B\textbf{x} = \textbf{c}$ требует существенно мень- шего объема вычислений по сравнению с решением системы $A\textbf{x} = \textbf{b}$. Решение системы $B\textbf{x} = \textbf{b}$ принимаем в качестве первого приближения $\textbf{x}^1$ к решению. Разность x − x1 удовлетворяет системе уравнений

 \[ A(\textbf{x} - \textbf{x}') = \textbf{b} - A\textbf{x}^1.\]
 
 Вместо решения этой системы находим решение системы
  \[ B\textbf{r}^1 = \textbf{b}  A\textbf{x}'\]
  
и полагаем $\textbf{x} = \textbf{x}^1 + \textbf{r}^1$ . Таким образом, каждое приближение находится из
предыдущего по формуле
\[
\textbf{x}^{n+1} =\textbf{x}^n + B^{-1} (\textbf{b}- A\textbf{x}^n )=(E - B^{-1}A)\textbf{x}^n +B^{-1}\textbf{b}.
\]
 Если матрицы A и B достаточно близки, то матрица $
E - B^{-1}A$имеет малую норму и такой итерационный процесс быстро сходится (см. также \textsection 10).
\end{small}

\end{adjustwidth}

\indent Значительно более редкой, чем задача решения системы уравнений, является задача обращения матриц. Для обратной матрицы$ X = A^{-1}$ имеем равенство $AX = BDX = E$. Таким образом, для нахождения матрицы X достаточно последовательно решить две матричные систе- мы $BY = E$, $DX = Y$ . Нетрудно подсчитать, что при нахождении на таком пути матрицы $A^{-1}$ общий объем вычислений составит $N_{2} \sim 2m^3$ арифметических операций.
\indent В случае необходимости уточнения приближения к обратной мат- рице могут производиться при помощи итерационного процесса $X_{k }= X_{k-1}(2E-AX_{k-1})$. Для исследования сходимости итерационного процесса рассмотрим матрицы $G_{k} = E - AX_{k}$. Имеем равенство
\[
G_{k} =E - AX_{k} =E-AX_{k-1}(2E-AX_{k-1})=(E-AX_{k-1})^2 =G_{k-1}^2.
\]
Отсюда получаем цепочку равенств
\[
G_{k} = G_{k-1}^2 = G_{k-2}^4 = ... = G_{0}^{2^{k}}.
\]
Поскольку
\[
A^{-1} - X_{k} = A^{-1}(E - AX_{k})=A^{-1}G_{k}=A^{-1}G_{0}^{2^{k}},
\]

\noindent то имеем оценку
\[ \Vert A^{-1} - X_{k} \Vert \leq \Vert A^{-1} \Vert \cdot \Vert G_{0} \Vert ^{2^{k}} \ \] 

Таким образом, при достаточно хорошем начальном приближении, т.е.
если $\Vert E - AX_{0} \Vert \leq 1$, этот итерационный процесс сходится со скоростью более быстрой, чем геометрическая прогрессия.

\begin{center}
    \section*{\textsection2. Метод отражений
}
\end{center}
\noindent В настоящее время разработано так много точных методов численного решения систем линейных алгебраических уравнений, что даже простое перечисление их затруднительно. Большинство этих методов, как и метод исключения Гаусса, основано на переходе от заданной системы $A\textbf{x} = \textbf{b}$ к новой системе $CA\textbf{x}=C\textbf{b}$ такой, что система $Bx=\textbf{d}$, где $B=CA$ и $\textbf{d}= C\textbf{b}$, решается проще, чем исходная. При выборе подходящей матрицы $C$ нужно учитывать по крайней мере следующие два фактора. Во-первых, ее вычисление не должно быть чересчур сложным и трудоемким. Во- вторых, умножение на матрицу $C$ не должно в каком-то смысле портить матрицу $A$ (мера обусловленности матрицы не должна меняться сильно (см. \textsection  11)).
\\ \indent
Этим условиям в определенной степени удовлетворяет описываемый ниже \textit{метод отражений}. Среди методов, требующих для своей реализа- ции $N \sim 4m^3/3$ операций, этот метод в настоящее время рассматривается как один из наиболее устойчивых к вычислительной погрешности. Среди методов, требующих для своей реализации $N \sim 4m^3/3$  операций, как наи- более устойчивый к вычислительной погрешности рассматривается \textit{метод вращений}.
\\ \indent Рассмотрим случай вещественной матрицы A. Если \textbf{w} — некоторый вектор-столбец единичной длины, $(\textbf{w}, \textbf{w})$ = 1, то матрицу

\[
U = E - 2\textbf{w}\textbf{w}^T
\]
называют \textit{матрицей отражений}. Под $\textbf{ww}^T$ здесь понимается матрица, являющаяся произведением вектора-столбца $\textbf{w}$ на вектор-строку $\textbf{w}^T$, т.е. $\textbf{w}\textbf{w}^T$ = $(w_{ij})$, где $w_{ij}$ = $w_{i}w_{j}$. Из определения следует, что $\textbf{w}\textbf{w}^T$ — сим- метричная матрица. 
\\ 
\indent Непосредственной проверкой убеждаемся, что $U = U^T$ и
\\
$UU^T = (E - 2\textbf{w}\textbf{w}^T)^T(E - 2\textbf{w}\textbf{w}^T)^T = E - 2\textbf{w}\textbf{w}^T - 2 \textbf{w}\textbf{w}^T +4 \textbf{w}\textbf{w}^T\textbf{w}\textbf{w}^T =E;$
\\
здесь мы воспользовались тем, что
\[\textbf{w}^T\textbf{w} = (\textbf{w},\textbf{w}) = 1.\]
\\
Таким образом, матрица $U$ —симметричная и ортогональная.
\\
\begin{adjustwidth}{1cm}{0cm}

\begin{small}
Напомним один факт из алгебры. Пусть $U$ и $B$ — две матрицы порядка $m$, $B$ —
многочлен от $U$, $B$ = $P_{l}(U)$. Тогда можно переупорядочить их собственные BU
значения так, что $\lambda_{j}^B = P_{l}(\lambda_{j}^U)$ при $j = 1,..., m$.

\end{small}

\end{adjustwidth}
\\
\indent Поскольку $U$ симметрична и $U^2$ = $UU^T = E$, а все собственные числа $E$ равны 1, то все собственные числа матрицы $U$ удовлетворяют условию $\lambda){U}^2 = 1$, т.е. равны или $+1$ или $-1$.
\\ \indent
Собственному значению −1 отвечает собственный вектор $w$. В самом
деле,
\[
U\textbf{w} = \textbf{w} - 2\textbf{w}\textbf{w}^T\textbf{w}=\textbf{w} - 2 \textbf{w} = - \textbf{w}.
\]

\indent
Все векторы, ортогональные вектору w, являются собственными. Им соответствует собственное значение, равное $+1$. Действительно, пусть $(v, w) = 0$. Тогда имеем
\[
U\textbf{v} = \textbf{v} - 2\textbf{w}\textbf{w}^T\textbf{v}=\textbf{v} - 2 \textbf{w}(\textbf{w}, \textbf{v}) =  \textbf{v}.
\]

Представим произвольный вектор $y$ в виде $\textbf{y} = \textbf{z} + \textbf{v}$, где $\textbf{z} = \gamma \textbf{w}$, $(\textbf{v}, \textbf{w}) = 0$. Для этого следует взять в качестве $\textbf{z}$ проекцию вектора $\textbf{y}$ на вектор $\textbf{w}$, т.е. $\textbf{z} = (\textbf{y},\textbf{w})\textbf{w}$, и $\textbf{v} = \textbf{y}−(\textbf{y},\textbf{w})\textbf{w}$. Вследствие (2) и (3) имеем $U_{\textbf{y}} = −\textbf{z} + \textbf{v}$. Таким образом, $U_{\textbf{y}}$ есть зеркальное отражение вектора $\textbf{y}$ относительно гиперплоскости, ортогональной вектору $\textbf{w}$.
Используя геометрические свойства матрицы отражений, нетрудно решить следующую задачу: подобрать вектор $\textbf{w}$ в матрице отражений так, чтобы заданный вектор $y \neq 0$ имел в результате преобразования $U_{\textbf{y}}$ матрицей отражения $U = E − 2ww^T $направление заданного единичного вектора $\textbf{e}$.
\\
\indent
Так как U — ортогональная матрица, а при ортогональных преобразо- ваниях длины векторов сохраняются, то мы должны иметь $U_{\textbf{y}} = \alpha \textbf{e}$ или
$U_{\textbf{y}} = \alpha -\textbf{e}$ , где $\alpha = (y, y)$. Поэтому направление, перпендикулярное плоскости отражения, будет определяться либо вектором $\textbf{y} − \alpha \textbf{e}$, либо вектором $\textbf{y}+\alpha \textbf{e}$ (см. рис. 6.2.1).
\begin{wrapfigure}{r}{0.3\textwidth}
    \includegraphics[scale=0.6]{something 2023-11-06 в 23.53.57.png}
\end{wrapfigure}
\indent Таким образом, векторы $\textbf{w}_{1} = \pm \rho_{1}^{-1}(\textbf{y} - 
\alpha \textbf{e})$ или $\textbf{w}_{2} = \pm \rho_{2}^{-1}(\textbf{y} + 
\alpha \textbf{e})$  где $\rho_{1} = \sqrt{
(\textbf{y} - \alpha \textbf{e}, y - \alpha \textbf{e})}$, $\rho_{2} = \sqrt{
(\textbf{y} + \alpha \textbf{e}, y + \alpha \textbf{e})}$, будут искомыми. Ясно, что данный процесс всегда осуществим. Если векторы $\textbf{y}$ и $\textbf{e}$ коллинеарны, а в этом случае либо $\rho_{1}$, либо $\rho_{2}$ будет равно нулю, то никаких отражений делать не надо
\\
\indent
Матрицы отражения нашли широкое
применение при численном решении раз-
личных задач линейной алгебры (в частности, в рассматриваемой нами задаче приведения матрицы системы уравнений к треугольному виду). 
\\

\chapter{\textbf{Лемма.}}
\textit{ Произвольная квадратная матрица может быть представлена в виде произведения ортогональной и верхней треугольной матриц.}

\\ \\ 
\textit{Доказательство. } Пусть дана квадратная матрица порядка $m$. Будем приводить ее к правой треугольной матрице путем последовательного умножения слева на ортогональные матрицы. На первом шаге приведения рассмотрим в качестве вектора $\textbf{y}$ из предыдущего рассуждения первый столбец матрицы $A$:
\[\textbf{y}_{l} = (a_{11}, ..., a_{m1})^T.\]

\noindent Если $a_{21} = a_{31} = ... = a_{m1} = 0$, то переходим к следующему шагу, положив $A^{(1)} = A, U_{1} = E$ и введя обозначения $a_{ij}^{(1)} = a_{ij}$ . В противном случае
умножаем матрицу $A$ слева на матрицу отражения $U_{1} = E_{m} - 2\textbf{w}_{1}\textbf{w}_{1}^T$ , где $w_{1}$ подбирается так, чтобы вектор $U_{1}\textbf{y}_{1}$ был коллинеарен вектору $\textbf{e}_{1} = (1, 0, ... , 0)^T$ . Здесь и далее $\textbf{E}_{q}$ — единичная матрица размерности $q$.
\\
\indent
На этом первый шаг закончен, и на следующем шаге будем рассмат- ривать матрицу $A^{(1)}$ с элементами $a_{ij}^{(1)}$, которая либо равна $A$, если имеет
место первый случай, либо $A^{(1)}$ $= U_{1}A$, если имеет место второй случай. Пусть мы уже осуществили $l−1 > 0 $шагов и пришли к матрице $A^{(l−1)}$
с элементами $a_{ij}^{(l−1)}$такими, что $a_{ij}^{(l−1)} = 0$ при $i > j$, $j = 1, 2,..., l−1$. В 
пространстве $R_{m−l+1} $векторов размерности $m − l + 1$ рассмотрим вектор
\[
\textbf{y}_{l} = \left(a_{l,l}^{(l-1)}, a_{l + 1,l}^{(l-1)}, ..., a_{m,l}^{(l-1)}\right)^T.
\]
Если$a_{l + 1,l}^{(l-1)} = a_{l + 2,l}^{(l-1)} =... = 0$, то переходим к следующему шагу, полагая
$A^{(l)} = A^{(l−1)}, U_{l} = E.$ В противном случае строим матрицу отражения $V_{l} = E_{m-l+1} - 2\textbf{w}_{l}\textbf{w}_{l}^T $(размеры матрицы $V_{l}$ и вектора $\textbf{w}_{l}$ равны $m - l + 1)$, переводящую вектор $\textbf{y}_{l}$ в вектор, коллинеарный $\textbf{e}_{l} = (1, 0, ..., 0)^T \in  R_{m-l+1}$, и переходим к матрице
\[A^{l} = U_{l}A^{(l-1)};\]
здесь
$ U_{l} = 
\left(
\begin{array}{ccc}
E_{l-1} & 0 \\
0 & V_{l}
\end{array}
\right)$ . Ясно, что процесс всегда осуществим, и после
$(m-1)$-го \\ 
шага мы приходим к матрице
\[
A^{(m-1)} = U_{m-1}U_{m-2}...U_{1}A, 
\]
имеющей правую треугольную форму.
\\
\indent
Если обозначить $U_{m-1}U_{m-2}...U_{1} =U$, то из последнего равенства сле-
дует, что $A = U^T A^{(m−1)}$ , где $U^T$ — ортогональная, а $A^{(m-1)}$ — правая тре- угольная матрицы. Лемма доказана.
\\  \\ 
\indent
Вернемся к решению системы $A\textbf{x} = \textbf{b}$. С помощью указанных пре- образований отражения последовательно приводим ее к эквивалентному
\[
A^{(m-1)}\textbf{x} = U \textbf{b},
\]
где $A^{(m-1)}$ — правая треугольная матрица. Если все диагональные эле- менты $A^{(m-1)}$ отличны от нуля, то последовательно находим $x_{x}, ..., x_{1}$. Если же хотя бы один из диагональных элементов равен нулю, то по- следняя система вырождена и в силу эквивалентности вырождена и ис- ходная система.

\\ \\ 
\noindent
\chapter{\textbf{Задача 1.}}
Получить асимптотику числа операций метода отражений при\\
$m \to \infty $
\\
\indent
Рассмотрим случай системы уравнений $A\textbf{x} = \textbf{b}$ с комплексными $A$ и
$\textbf{b}$. Пусть
\[
A = A_{1} + iA_{2}, 
 \textbf{b} = \textbf{b}_{1} + i \textbf{b}_{1},  \textbf{x} = \textbf{x}_{1} + i\textbf{x}_{2}.
\]
Исходная система уравнений равносильна системе
\[
C\textbf{y} = \textbf{d}
\]
с вещественными $C$ и $\textbf{d}:$
\[
C = 
\left(
\begin{array}{ccc}
A_{1} & -A_{2} \\
A_{2} & A_{1}
\end{array}
\right) ,  \; \;  \; \;
\textbf{d} = 
\left(
\begin{array}{ccc}
\textbf{b}_{1}  \\
\textbf{b}_{2}
\end{array}
\right) ,   \; \;  \; \;
\textbf{y} = 
\left(
\begin{array}{ccc}
\textbf{x}_{1}  \\
\textbf{x}_{2}
\end{array}
\right)
.
\]

Поэтому вместо непосредственного решения исходной задачи можно пе- рейти к решению задачи $(4)$ и 
применить для решения последней метод отражений.
\\
\indent
Однако возможен и другой путь—непосредственное применение мето- да отражений к исходной системе $A\textbf{x} = \textbf{b}$. Здесь матрица отражения $U = E - 2\textbf{w}\textbf{w}^{*}, w^{*} = (\bar w_{1}, ..., \bar w_{m})^T $ будет унитарной с собственными зна- чениями вида $\lambda_{U} = e^{i \phi}$. (Через z ̄ обозначено комплексное число, сопря- женное с $z$).
\\ \\ 
\chapter{\textbf{Задача 2. }} 
Перенести метод отражений на случай комплексных матриц.
\\ \\
\chapter{\textbf{Задача 3. }} 
Исследовать метод отражений в случае его применения для
решения систем уравнений с ленточной матрицей.
\\ 
\begin{center}
    \section*{\textsection3  Метод простой итерации
}
\end{center}

Простейшим итерационным методом решения систем линейных уравнений является \textit{метод простой итерации}. Система уравнений
\[ A\textbf{x} = \textbf{b}\eqno(1)\]
преобразуется к виду
\[ \textbf{x} = B \textbf{x} + c, \eqno(2)\]
и ее решение находится как предел последовательности
\[ \textbf{x}^{n + 1} = B \textbf{x}^{n} + \textbf{c}. \eqno(3)\]
\indent
Всякая система
\[ \textbf{x}=  \textbf{x} + D(A\textbf{x} - \textbf{b}) \eqno(4)\]

имеет вид (2) и при $\det D \neq 0 $ эквивалентна системе (1). В то же время всякая система (2), эквивалентная (1), записывается в виде (4) с матри- цей $D = (E - B)A^{-1}$.
\\ \\ 
\chapter{\textbf{Теорема }}
(о достаточном условии сходимости метода простой итерации). Если $\|B\| < 1 $, \textit{то система уравнений } (2) \textit{имеет единственное решение и итерационный процесс} (3) \textit{ сходится к решению со скоростью геометриче-
ской прогрессии.}
\\ 
\noindent \textit{Доказательство. } 
Для всякого решения системы (2) имеет место $ \| \textbf{x} \| \leqslant \|B\| \| \textbf{x} \| + \| \textbf{c} \| $, поэтому справедливо неравенство $\|\textbf{x}\|(1 - \| B \| ) \leqslant \| \textbf{c} \|$ или $\| \textbf{x} \| \leqslant (1 - \|B\|)^{-1}\|\textbf{c} \|$. Отсюда следует существование и единственность решения однородной системы $\textbf{x} = B\textbf{x}$, а следовательно, и системы (2). Пусть $\textbf{X}$—решение системы (2). Из (2) и (3) получаем уравнение отно- сительно погрешности $\textbf{r}^{n} = \textbf{x}^{n} - \textbf{X}:$

 \[
 \textbf{r}^{n + 1} = B\textbf{r}^{n}.
 \eqno(5)
 \]
 Из (5) получаем равенство
  \[
 \textbf{r}^{n} = B^n\textbf{r}^{0}.
 \eqno(6)
 \]
 Отсюда следует, что 
 $\| \textbf{r}^n\| \leqslant \| B\| ^n \| \textbf{r}^0\| \to 0.$
 \; Теорема доказана. \\ \\
 \indent 
 Качество итерационного процесса удобно характеризовать скоростью убывания отношения погрешности после $n$ итераций к начальной погреш-
ности:
\[
s_{n} = \sup_{\textbf{x}^0 \neq \textbf{X}} \frac{\| \textbf{r}^n \|}{\| \textbf{r}^0 \|}
=  \sup_{\textbf{r}^0 \neq 0} \frac{\| B\textbf{r}^0 \|}{\| \textbf{r}^0 \|}
= 
\| B^n \|.
\]
Можно гарантировать, что величина $s_{n} \leqslant \varepsilon$, если $\|B\|^{n} \leqslant \varepsilon$, т. е. при
\[
n \geqslant n_{\varepsilon} = \ln (\varepsilon ^{-1} ) / \ln ( \| B \|^{-1}).
\eqno(7)
\]
\indent 
Если существуют постоянные $\gamma _{\alpha \beta}$, $\gamma _{\beta \alpha }$ такие, что при $\textbf{x} \neq 0$
\[
\| \textbf{x} \|_{\beta} / \|\textbf{x} \|_{\alpha} \leqslant \gamma _{\alpha \beta}, 
\| \textbf{x} \|_{\alpha} / \|\textbf{x} \|_{\beta} \leqslant \gamma _{\beta \alpha},
\]
то нормы  $\| \textbf{x} \|_{\alpha}$ и $\| \textbf{x} \|_{\beta}$называются \textit{эквивалентными}. Имеем
\[
\| \textbf{r}^n_{\beta} \| \leqslant \gamma_{\alpha\beta}
\| \textbf{r}^n \|_{\alpha} \leqslant \gamma_{\alpha\beta}
\|B\|^n_{\alpha} 
\|\textbf{r}^0 \| \leqslant \gamma_{\alpha\beta}\gamma_{\beta\alpha}
\|B\|^n_{\alpha} 
\|\textbf{r}^0 \|_{\beta}
.\]

\noindent
Таким образом, если условие доказанной теоремы выполнено для нормы    $\| \cdot\| _{\alpha}$, то утверждение справедливо относительно любой эквивалентной ей нормы. \\
\indent Любые две нормы в конечномерном пространстве являются эквива- лентными. В частности, нормы $\| \textbf{x}_{1}\|$, $\| \textbf{x}_{2}\|$, $\| \textbf{x}_{3}\|$, вычисляемые соответ- ственно по формулам (2), (3), (4), приведенным во введении к настоя- щей главе, эквивалентны между собой вследствие справедливости цепоч- ки неравенств
\[
\| \textbf{x}_{\infty} \| \leqslant \|\textbf{x}\|_{2}\leqslant \|\textbf{x}\| _{1} \leqslant m\|\textbf{x}\|_{\infty}.
\]
\\
\chapter{\textbf{Лемма. }} \textit{Пусть все собственные значения} $\lambda_{i}$ \textit{матрицы} $B$ \textit{лежат в круге }$\mid\lambda\mid \leqslant q$, \textit{причем собственным значениям, по модулю равным} $q$, \textit{соответ- ствуют жордановы клетки размерности 1. Тогда существует матрица} $	\Lambda = D^{-1}BD$ \textit{с нормой} $\| \Lambda \|_{\infty} \leqslant q$.
\\ \\ 
\noindent \textit{Доказательство.} Положим $\eta = q - 
\max\limits_{{\mid\lambda_{i}\mid < q}}\mid\lambda_{i}\mid.
$ Собственными значениями матрицы
$\eta^{-1}B$ будут $\eta^{-1}\lambda_{i}$. Преобразуем матрицу $\eta^{-1}B$ к жордановой
форме
\[ D^{-1}(\eta^{-1}B)D=
\left(
\begin{array}{cccc}
\eta^{-1}\lambda_{1} & \alpha_{12} & 0 & ... \\
0 & \eta^{-1}\lambda_{2} & \alpha_{2} & ...\\
. & .  & . & ...
\end{array}
\right) , 
\]
где $\alpha_{i,i+1}$ принимают значения 0 или 1. После умножения на $\eta$ получим
\[ \Lambda= D^{-1}BD=
\left(
\begin{array}{cccc}
\lambda_{1} & \alpha_{12}\eta & 0 & ... \\
0 & \lambda_{2} & \alpha_{23}\eta & ...\\
. & .  & . & ...
\end{array}
\right) , 
\]
Если $\mid\lambda_{i}\mid=q$,  , то согласно условиям леммы, $\alpha_{i, i+1}= 0$.
Отсюда следует,
что $\mid \lambda_{i} \mid + \mid \alpha_{i,i+1}\eta \mid = q $. Если $\mid\lambda_{i} < q\mid$, то 
\[
\mid \lambda_{i} \mid + \mid \alpha_{i,i+1}\eta \mid
\leqslant \max\limits_{{\mid\lambda_{i}\mid} < q} \mid\lambda_{i}\mid+\eta = q.
\]
Таким образом, $\| \Lambda \| _{\infty} = \max\limits_{i} (\mid \lambda_{i} \mid + \mid \alpha_{i,i+1}\eta \mid) \leqslant q.$
\\
\chapter{\textbf{Теорема }}
(о необходимом и достаточном условии сходимости метода про- стой итерации). \textit{Пусть система} (2)\textit{ имеет единственное решение. Итерационный процесс} (3)
\textit{сходится к решению системы} (2) \textit{ при любом начальном приближении тогда и только тогда, когда все собственные значения матрицы} $B$ \textit{по модулю меньше 1.}
\\ \\ 
\noindent 
\textit{Доказательство. Достаточность. }
Возьмем произвольное $q$ в пределах \; \; $\max\limits_{i} \mid\lambda_{i}\mid < q < 1$. Условие леммы выполнено по отношению к этому $q$,
поэтому существует матрица $D$ такая, что $\| \Lambda \|_{\infty} \leqslant q$ при 
$\Lambda = D^{-1}BD.$
Поскольку $B = D\Lambda D^{-1},$ то 
\[
B^n=D\Lambda D^{-1}D...D^{-1}D\Lambda D^{-1}=D\Lambda ^{n} D^{-1}.
\]
Поэтому
\[
\|B^n\|_{\infty} \leqslant \|D\|_{\infty} \|D^{-1}\|_{\infty}q^{n} \to 0
\]
и
\[
\|\textbf{x}^n -\textbf{X}\|_{\infty}\leqslant
\|D\|_{\infty} \|D^{-1}\|_{\infty}q^{n} \|\textbf{x}^0 - \textbf{X} \|_{\infty} \to 0
\eqno(8) \]
при $n \to \infty$. Следовательно, и $\|\textbf{x}^n -\textbf{X}\|_{1}, \|\textbf{x}^n -\textbf{X}\|_{2} \to 0$.
\\ 
\begin{adjustwidth}{1cm}{0cm}

\begin{small}
Если $\chi _{i}$ — координатные орты, $\textbf{x} = (x_{1}, ..., x_{m})^T$, то $\textbf{x} = \sum\limits_{i} x_{i} \chi_{i}$ некоторая норма, тогда
\[
\|\textbf{x} \| \leqslant \sum\limits_{i} \mid x_{i} \mid \|\chi_{i}\|\leqslant \|\textbf{x}\|_{\infty} \sum\limits_{i} \|\chi_{i}\|.
\]
Поэтому при любой норме $\| \cdot \|$ имеем

\[ 
\|\textbf{x}^n -\textbf{X}\| \leqslant \left(\sum\limits_{i} \|\chi_{i}\|\right) \|D\|_{\infty} \|D^{-1}\|_{\infty}q^{n} \|\textbf{x}^0 - \textbf{X} \|_{\infty} \to 0
\eqno(9)
\]

\noindent Соотношения (8), (9) означают также, что любые нормы погрешности убывают
быстрее любой геометрической прогрессии со знаменателем, большим $\max\limits_{i} \mid \lambda_{i}\mid .$

\end{small}
\end{adjustwidth}

\indent 

\textit{Необходимость.} Пусть $\mid \lambda_{i}\mid \geqslant 1$ и $\textbf{e}_{1}$ — соответствующий собственный вектор матрицы $B$. Тогда при начальном приближении $\textbf{x}^0 = \textbf{X} + c\textbf{e}^1, c \neq 0 $ имеем

\[
\textbf{r}^{0} = c\textbf{e}_{1} \; \; \; \text{и} \; \; \;  \textbf{r}^n = \lambda_{l}^nc\textbf{e}_{1} \nrightarrow 0 \; \; \;  \text{при } \; \; \;   n \to \infty.
\]
\\
\chapter{\textbf{Задача 1. }} 
Пусть все собственные значения матрицы $B$, за исключением простого $\lambda_{1} = 1$, лежат внутри единичного круга и система (2) имеет решение $X$. Решением системы будут также все $\textbf{x} = \textbf{X} + c\textbf{e}_{1}$. Доказать, что итерационный процесс (3) сходится к одному из таких решений.

\\ \\

\begin{center}
    \section*{\textsection4. Особенности реализации метода простой
итерации на ЭВМ
}
\end{center}
\noindent
Если все собственные значения матрицы B лежат внутри единичного круга, то может показаться, что не возникает никаких проблем отно- сительно поведения метода в реальных условиях ограниченности поряд- ков чисел в ЭВМ и присутствия округлений. В обоснование этого ино- гда приводят следующий довод: возмущения приближений в результате округлений равносильны возмущениям начальных условий итерационно- го процесса. Поскольку процесс сходящийся, «самоисправляющийся», эти возмущения в конце концов затухнут, и будет получено хорошее прибли- жение к решению исходной задачи.
\\
\indent
Однако при решении некоторых систем возникала следующая ситуа- ция. Все собственные значения матрицы B лежали в круге $\mid\lambda\mid \; \leqslant 1/2$, а итерационный процесс останавливался после некоторого числа итера- ций из-за переполнения порядков чисел в ЭВМ. В других случаях та- кого переполнения не происходило, но векторы $\textbf{x}^n$, получаемые при вы- числениях, не сходились к решению. Последний случай особенно опасен по следующей причине. Можно необоснованно решить, что при условии $\mid\lambda_{i}\mid \leqslant 1/2$ какое-то определенное число итераций, например 100, заве- домо достаточно для получения решения с требуемой точностью. Затем производим эти 100 итераций и рассматриваем полученный результат как требуемый. Поэтому наличие подобных явлений послужило толчком к бо- лее детальному исследованию итерационных процессов и формированию новых понятий в теории операторов.
\\
\indent
Чтобы понять сущность явления, полезно построить пример, где это явление прослеживается в явном виде. В качестве модели выберем ите- рационный процесс, соответствующий двухдиагональной матрице

\[
B_{0} = 
\left(
\begin{array}{ccccc}
\alpha & \beta & 0 & ... & 0 \\
0 & \alpha & \beta & ... & 0 \\
. & . & . & ... & .\\
0 & 0  & 0 & ...& \alpha
\end{array}
\right)
.
\]
При возведении матрицы $B0$ в степень $n$, получается треугольная мат-
рица
\[
B_{0}^n=(b_{ij}^{(n)}=
\left(
\begin{array}{cccc}
\alpha^n & C_{n}^1\alpha^{n-1}\beta  & C_{n}^2\alpha^{n-2}\beta^2  & ...  \\
0 & \alpha^n & C_{n}^1\alpha^{n-1}\beta  & ... \\
. & .   & . & ... 
\end{array}
\right)
\]
с элементами $b_{ij}^{(n)} = C_{n}^{j-i}\alpha^{n-(j-i)}\beta^{j-i}$. Если $\textbf{r}^0 = (0, ..., 0, 1)^T$ , то

\[
\textbf{r}^n = B_{0}^n\textbf{r}^0 = (b_{1m}^{(n)}, ..., b_{mm}^{(n)})^T, \; \;
\|\textbf{r}^n\|_{1} = \sum\limits_{i=1}^{m}\mid b_{im}^{(n)}\mid.
\]
\indent При $n < m $ последнее выражение упрощается:
\[
\begin{split}
\|\textbf{r}^n\|_{1} &= \sum\limits_{i=1}^{m} C_{n}^{m-i}\mid\alpha\mid^{n-(m-i)}\mid\beta\mid^{m-i} 
= \\
&=
 \sum\limits_{k=0}^{m-1} C_{n}^{k}\mid\alpha\mid^{n-k)}\mid\beta\mid^{k} = 
\sum\limits_{k=0}^{n} C_{n}^{k}\mid\alpha\mid^{n-k)}\mid\beta\mid^{k} = \left(\left|\alpha\right| + \mid\beta\mid\right)^n.
\end{split}
\]
\\
Рассмотрим случай $\left|\alpha\right| < 1, \left|\alpha\right|
+ \left|\beta\right| > 1, \left|\beta/(1-\alpha)\right| < 1.$ Пусть $\textbf{c} = \textbf{c}^0=(0,..., 0, 1)^T$. Непосредственно проверяется, что при таком c решением рассматриваемой системы будет
\[
\textbf{X}^0 = \left(\frac{1}{1- \alpha}\right)
\left( \frac{\beta}{1-\alpha}\right)^{m-1}, ..., \frac{1}{1-\alpha})^T.
\]
Справедлива оценка
\[
\|\textbf{X}^0\|_{1} \leqslant \omega,
\]
где 
\[
\omega = \frac{1}{\left|1 - \alpha \right|} 
\sum\limits_{k=0}^{\infty} \left|\frac{\beta}{1-\alpha} \right| ^k = \frac{1}{\left| 1- \alpha\right| \left( 1 - \left| \frac{\beta}{1-\alpha}\right| \right)} .
\]
\indent
При начальном приближении $\textbf{x}^0 = \textbf{X}^0 + \textbf{c}^0$ имеем $\textbf{r}^0 = \textbf{c}^0$ и, согласно
проводившимся выше построениям,

\[
\|\textbf{r}^n \|_{1} = (\left|\alpha \right| + \left|\beta \right| )^n \; \; \; \text{для} \; \; \; n < m.
\]

Выберем $m$ таким, чтобы число $\sigma = [(\left|\alpha\right| + \left|\beta\right|)^{m-1} - \omega]/m$ превосходило пределы, допустимые в ЭВМ. Из полученных ранее соотношений следует, что
\[
\|\textbf{x}^{m-1}\| _{\infty} \geqslant \|\textbf{x}^{m-1}\|_{1/m} \geqslant \left( \|\textbf{r}^{m-1}\|_{1} - \|\textbf{x}^{0}\| _{1} \right)/m \geqslant \sigma.
\]

\noindent Поэтому построенный пример обладает следующими свойствами: норма начального приближения невелика, итерационный процесс сходится при отсутствии округлений и ограничения на порядки чисел в ЭВМ, но оста- навливается не позднее чем при $n = m − 1$ из-за недопустимо больших значений компонент приближений.
\\
\indent Обратимся к реальной ситуации, когда на каждом шаге вычислений происходят округления. Рассмотрим подробнее случай, когда переполне- ние не происходит. Вместо $\textbf{x}^n$ получаются векторы $\textbf{x}^{*n}$, связанные соот- ношениями
\[
\textbf{x}^{*n+1}= B\textbf{x}^{*n} + \textbf{c} + \rho^n,
\]
\noindent где $\rho^{n}$суммарное округление на шаге итерации.
\\
\indent Отсюда и из (3.2) получается уравнение относительно погрешности
$\textbf{r}^{∗n} =\textbf{x}^{∗n}-\textbf{X}:$
\[
\textbf{r}^{∗n} = \rho^{n} + B\textbf{r}^{∗n}. \eqno(1)
\]
Выражая каждое $\textbf{r}^{*n}$ через предыдущее, получаем
\[
\textbf{r}^{*n} =\rho^{n} + B\textbf{r}^{∗n-1}= \rho^{n} + B(\rho^{n-2} + B\textbf{r}^{*n-2}=
\]
\[
=\rho^{n-1} +B\rho^{n-2}+...+B^{n-1}\rho^{0}+B^{n}\textbf{r}^{0}. \eqno(2) 
\]
\indent Как мы видели, норма $\|B_{0}^{n}$ при $\left|\alpha\right|+\left|\beta\right|>1$ имеет следующий
характер поведения: при малых $n$ она имеет тенденцию к возрастанию,
при больших $n$ стремится к нулю. (Можно показать, что максимальное
значение $\varphi(B_{0}) = \max\limits_{n}\|B_{0}^{n} \|$ достигается при значении $n = n_{0}$ порядка $m$.)
\\
При таком характере поведения норм $B^n$ может возникнуть следующая ситуация. Величина $\max\limits_{n}\|\textbf{x}^{*n} \|$ не настолько велика, чтобы происходило
\\
 переполнение и остановка ЭВМ; в то же время $\varphi(B)2^{-t} \gg R$, где $R$ —
максимально допустимая погрешность решения. Поэтому, как правило, при $n > n_{0}$ среди слагаемых в правой части (2) присутствует слагаемое $B^{n_{0}}\rho^{n-1-n_{0}}$ с нормой, много большей, чем $R$. В результате установление приближений $\textbf{x}^n$ с приемлемой точностью не происходит.
\\ 
\indent
Подведем некоторый итог проведенных построений. Матрицы высокой размерности обладают свойствами, существенно отличными от свойств матриц малой размерности. Кроме собственных значений у таких матриц есть почти собственные значения, т.е. $\lambda$ такие, что $\|A\textbf{x} - \lambda\textbf{x}\|\leqslant \varepsilon\|\textbf{x}\|$ при $\|\textbf{x}\| \neq 0$  \\ и очень малом $\varepsilon$.
\\ 

\begin{adjustwidth}{1cm}{0cm}

\begin{small}
Например, в случае матрицы $B_{0}$ при любом $\lambda_{0}$, лежащем в круге $\left|\alpha-\lambda\right|<\left|\beta\right|$,
можно построить вектор $\textbf{x}_{\lambda}$ такой, что $\|B_{0}\textbf{x}_{\lambda}-\lambda\textbf{x}_{\lambda}\|_{\infty}\leqslant \varepsilon_{\lambda}\|\textbf{x}_{\lambda}\|_{\infty}$, где $\varepsilon_{\lambda}=\left|\beta\right|\left|(\lambda-\alpha)/\beta\right|^{m}$.
Поведение степеней матрицы $B^{n}$ $n$ при порядка $m$ определяется во многом такими «почти собственными векторами» $\textbf{x}\lambda$ и «почти собственными значени- ями» $\lambda$. 
\end{small}
\end{adjustwidth}

\\
\noindent \textbf{Задача 2. } 
Построить «почти собственный вектор» $\textbf{x}_{\lambda}$, соответствующий значению $\varepsilon_{\lambda}$, приведенному выше.

\\ 

\begin{adjustwidth}{1cm}{0cm}
\begin{small}
Суммарная вычислительная погрешность $\rho_{n}=\sum\limits_{j=0}^{n-1}B^{n-1-j}\rho^{j}$ может оказаться
большой не только из-за большой величины отдельных слагаемых, но и из-за
того, что их много.
Пусть $B$ — симметричная матрица и$\|B\|_{2} = \max\limits_{i}\left|\lambda_{B}^{i}\right| = \lambda_{B}^{1} < 1, \textbf{e}^{1}$ соответствующий $\lambda_{B}^{1}$ нормированный собственный вектор. Предположим, что на
каждом $j-$м шаге происходит округление $\rho^{j} = \rho\textbf{e}^{1}$, где $\rho$ порядка $2^{2-t}$. Имеем равенство
\[
\rho_{n} = \rho\sum\limits_{j=0}^{n-1}(\lambda_{B}^{1})^{j}\textbf{e}^{1}
\]
\noindent
Поскольку число итераций берется таким, что $\|B^{n}\| \gg 1, $а $\|B^{n}\|=(\lambda_{B}^{1})^{n}$ то можно считать, что $\|\rho_{n}\| \approx \rho/(1-\lambda_{B}^{1})$. Таким образом, если $\lambda_{B}^{1}$ близко к 1, то суммарное влияние округлений на шагах интегрирования может оказаться довольно большим.
\\
Покажем, что вычислительная погрешность такого порядка является неизбежной. Предположим, что вместо системы (3.2) решается система $\textbf{X} = B\textbf{X} + \textbf{c} + \rho\textbf{e}_{1}$. Разность $\textbf{X} - \textbf{x}$ решений этих систем удовлетворяет соотношению $(\textbf{X} - \textbf{x}) = B(\textbf{X} - \textbf{x}) + \rho\textbf{e}_{1}$, отсюда $\textbf{X} - \textbf{x} = (E - B)^{-1}\rho\textbf{e}_{1}$ Поэтому погрешность порядка $(1 - \lambda_{B}^{1})^{-1}\rho$ является неустранимой; возмущение прибли- жений, создаваемое в ходе итераций, сравнимо с неустранимой погрешностью.
\end{small}
\end{adjustwidth}

\\ \\

\begin{center}
    \section*{\textsection5. $\delta^{2}$-процесс практической оценки погрешности
и ускорения сходимости
}
\end{center}
\noindent
Рассмотрим вопрос об оценке погрешности приближенного решения си- стемы уравнений. Если 
$\textbf{X}^{*}$ — приближенное решение системы $A\textbf{X} = \textbf{b}$, а $\textbf{X}$—точное решение этой системы, то можно написать равенство
\[
\|\textbf{X}^{*} - \textbf{X}\|=\|A^{-1}(A\textbf{X}^{*}-\textbf{b})\|\leqslant \|A^{-1}\|\|A\textbf{X}^{*}-\textbf{b}\|,
\]

которое редко применяется из-за сложности оценки $\|A^{-1}\|.$ Поэтому при практическом анализе погрешности приближений, получаемых итераци- онными методами, обычно вместо этой оценки используется рассматри- ваемая далее нестрогая, но более простая оценка погрешности, которая строится на основании дополнительной информации, получаемой в про- цессе вычислений.
\\
\indent
Примем следующий 
\textit{критерий разумности практической оценки погреш- ности:} $\textbf{v}^{n}$ принимается за практическую погрешность приближения $\textbf{x}^{n}$, стремящегося к $\textbf{X}$ при $\n \to \infty$, если
\[
\| \textbf{v}^{n}-(\textbf{x}^{n} - \textbf{X})\| \textbf{/} \| \textbf{x}^{n} - \textbf{X}\| \to 0 \; \; \; \text{при } \; \; \; n \to \infty \eqno{(1)}
\]
\noindent
Ясно, что тогда $\|\textbf{v}^{n}\| \sim \|\textbf{x}^{n} - \textbf{X} \|$.
\\
\indent Рассмотрим метод простой итерации $\textbf{x}^{n+1} = B\textbf{x}^{n} + \textbf{c}$ Для краткости
изложения ограничимся случаем, когда матрица $B$ простой структуры (т.е. ее жорданова форма диагональна и поэтому она обладает полной системой собственных векторов).
\\
\indent
Пусть $\lambda_{i},  i = 1,...,m,$—собственные значения матрицы $B$, занумеро-
ванные в порядке убывания $\left|\lambda_{i}\right|,$ причем $1 > \left|\lambda_{1}\right| > \left|\lambda_{2}\right| \geqslant \left|\lambda_{3}\right|\geqslant...\geqslant \left|\lambda_{m}\right|,$
а $\textbf{e}_{i}, \|\textbf{e}_{i}\| = 1, $ — соответствующие собственные векторы, образующие полную систему. Разложим вектор $\textbf{r}^{0}$ по базису $\textbf{e}_{i}: \textbf{r}^{0} = \sum  c_{i}\textbf{e}_{i}$. Тогда
\[
\textbf{r}^{0} = \textbf{x}^{n} - \textbf{X} = B^{n}\textbf{r}^{0} = \sum c_{i}\lambda_{i}^{n}\textbf{e}_{i} = c_{1}\lambda_{i}^{n}\textbf{e}_{1}+O(\left| \lambda_{2}\right|^{n}).
\eqno{(2)}
\]
\begin{adjustwidth}{1cm}{0cm}

\begin{small}

Здесь и далее выражение $\textbf{x}^{n} = \textbf{y}^{n} + O(\varepsilon_{n})$ имеет следующий смысл:
\[
\| \textbf{x}^{n} 
- \textbf{y}^{n} \| = O(\varepsilon_{n}) \; \; \; \text{при} \; \; \; n \to \infty.
\]
Далее в этом параграфе $\|\textbf{x}\|$ — это $\|\textbf{x}_{2}\|$.
\end{small}
\end{adjustwidth}

\\ \\  \indent 
Укажем способ построения приближения к вектору $\textbf{w}^{n} = c_{1}\lambda_{1}^{n}\textbf{e}_{1}$ на основании информации, получающейся в ходе вычислений. Согласно (2) имеем

\[
\textbf{x}^{n-2} - \textbf{X} = \textbf{w}^{n}\lambda_{1}^{-2} + O(\left|\lambda_{2}\right|^{n}),
\]
\[
\textbf{x}^{n-1} - \textbf{X} = \textbf{w}^{n}\lambda_{1}^{-1} + O(\left|\lambda_{2}\right|^{n}),
\]
\[
\textbf{x}^{n} - \textbf{X} = \textbf{w}^{n} + O(\left|\lambda_{2}\right|^{n}).
\]
\\
Вычитая друг из друга соседние соотношения, получим
\\
\[
\textbf{x}^{n-1} - \textbf{x}^{n-2} = \textbf{w}^{n}(1 - \lambda_{1}^{-1})\lambda_{1}^{-1} + O(\left|\lambda_{2}\right|^{n}),
\]
\[
\textbf{x}^{n} - \textbf{x}^{n-1} = \textbf{w}^{n}(1 - \lambda_{1}^{-1})\lambda_{1}^{-1} + O(\left|\lambda_{2}\right|^{n}).
\eqno{(3)}
\]
\begin{adjustwidth}{1cm}{0cm}
\begin{small}
Положим
\[
\lambda_{1}^{(n)} = \frac{(\textbf{x}^{n} - \textbf{x}^{n-1}, \textbf{x}^{n} - \textbf{x}^{n-1})}{(\textbf{x}^{n-1} - \textbf{x}^{n-2}, \textbf{x}^{n} - \textbf{x}^{n-1})}.
\]
\end{small}
\end{adjustwidth}

Воспользуемся соотношениями (4) и в предположении $c_{1} \neq 0$ поделим
числитель и знаменатель выражении для $\lambda_{1}^{(n)}$ на $\|\textbf{w}^{n}\|^{2} \left| 1 - \lambda_{1}^{-1} \right|^{2}\lambda_{1}^{-1}; $ в результате получим

\[
\lambda_{1}^{(n)} = \frac{\lambda_{1}+ O\left( \frac{\left| \lambda_{2}\right| ^{n}}{\|\textbf{w}^{n}\|} \right)}{1+ O\left(\frac{\left| \lambda_{2}\right| ^{n}}{\|\textbf{w}^{n}\|} \right)}.
\]
Поскольку
\[
\|\textbf{w}^{n}\| = \left| c_{1}\right|  \left \lambda_{1}\right| ^{n} \eqno{(5)}
\]
то 
\[
\lambda_{1}^{(n)} = \lambda_{1} + O\left(\left|\lambda_{2}/\lambda_{1}\right|^{n}\right). \eqno{(6)}
\]
Поделив второе из соотношений (3) на $1 -\left(\lambda_{1}^{(n)}\right)^{-1},$
получим
\[
\frac{\textbf{x}^n - \textbf{x}^{n-1}}{1 -\left(\lambda_{1}^{(n)}\right)^{-1} } = \textbf{w}^n \frac{1 -\lambda_{1}^{-1} }{1 -\left(\lambda_{1}^{(n)}\right)^{-1} } + O\left(\left|\lambda_{2}\right|^n\right) = \textbf{w}^n + \textbf{w}^n\frac{\lambda_{1} - \lambda_{1}^{(n)}}{\lambda_{1}(\lambda_{1}^{(n)}- 1 )} + O\left(\left|\lambda_{2}\right|^n\right).
\]
Из (5), (6) следует $\|\textbf{w}^n(\lambda_{1}-\lambda_{1}^{(n)}) \| = O\left( \left| \lambda_{2}\right|^n\right);$ поэтому
\[
\frac{\textbf{x}^n - \textbf{x}^{n-1}}{1 -\left(\lambda_{1}^{(n)}\right)^{-1} } = \textbf{w}^n+O\left( \left| \lambda_{2}\right|^n\right).
\]
Отсюда и из (2) получаем
\[
\textbf{x}^n - \textbf{X} = \textbf{v}^n + O(\left|\lambda_{2}\right|^n),
\]
где $\textbf{v}_{n} = \left(\textbf{x}^n - \textbf{x}^{n-1}\right)/(1 - \lambda_{1}^{n})^{-1}).$ Заметим, что согласно (3), (6) $\|\textbf{v}^n\| = \left|c_{1}\right|\left|\lambda_{1}\right|^n + O(\left|\lambda_{2}\right|^n).$ Из этих равенств вытекает, что $\textbf{v}^n$ удовле- творяет критерию (1), и поэтому его можно принять за практическую погрешность приближения $\textbf{x}^n$ .
\\
\indent В случае $c_{1} = ... = c_{l} = 0, c_{l+1} \neq 0$ проведенные рассуждения останутся в силе, если $\left|\lambda_{l+1}\right| > \left|\lambda_{l+2}\right|$ Во всех соотношениях следует заменить лишь $\lambda_{i}, c_{i}, \textbf{e}_{i}$ при $i = 1, 2$ на $\lambda_{l+i}, c_{l+i}, \textbf{e}_{l+i}.$ Описанный способ получения оценки приближенного решения называется $\delta^{2}$- \textit{процессом}.
\\
\indent Если положить $\textbf{y}^{n} = \textbf{x}^{n} - \textbf{v}^{n},$ то $\textbf{y}^n - \textbf{X} = O(\left|\lambda_{2}\right|^n)$, и поэтому $\textbf{y}^{n},$ вообще говоря, является лучшим начальным условием для последующих итераций по сравнению с $\textbf{x}^{n}$. Производя время от времени такие уточнения, иногда удается существенно уменьшить общее число итераций.
\\
\begin{adjustwidth}{1cm}{0cm}
\begin{small}
    Для справедливости приближенного равенства
    \[
    \textbf{x}^n - \textbf{X} \approx \textbf{v}^n
    \]

необходимо, чтобы в правой части равенства
\[
\textbf{x}^n - \textbf{X} = \sum \limits_{i}c_{i}\lambda_{i}^{n}\textbf{e}_{i}
\]

$\textbf{x}^n-\textbf{x}^{n-1}, \textbf{x}^{n-1} - \textbf{x}^{n-2}$ приблизительно пропорциональны и
\[
\mu_{n} = \frac{\left|(\textbf{x}^{n-1} - \textbf{x}^{n-2}, \textbf{x}^{n-1} - \textbf{x}^n)\right|}{\|\textbf{x}^{n-1} - \textbf{x}^{n-2}\| \|\textbf{x}^{n-1} - \textbf{x}^n \| }\approx 1.
\]
\end{small}
\end{adjustwidth}

Таким образом, условие $\mu_{1} \approx 1$ является необходимым для того, чтобы про- водившиеся ранее построения были справедливы. Поэтому его можно принять за условие практической применимости (7).
\\ \noindent
Например, возможна следующая схема метода простой итерации с примене-
нием $\delta^{2}$-процесса ускорения сходимости. Задаются некоторым $\eta'$ в пределах
$1>\eta′ >0$ и малым $\eta>0$.Если по ходу итераций оказалось,что $\mu_{n} \geqslant 1 - \eta$, 
то вычисляется $\textbf{v}^n$ и вектор $\textbf{y}^n$ принимается за начальное приближение для последующих итераций. Итерационный процесс прекращается, если $\mu_{n}$ и $\|\textbf{v}^n\| \leqslant \varepsilon$, где $\varepsilon$ требуемая точность.
\\
\noindent
Если $\eta$ очень мало, то условие $\eta_{n} \geqslant 1 - \eta$ будет выполняться только после
большого числа итераций, ускорение сходимости не будет иметь места. При
большом $\eta$ соотношения, положенные в основу наших построений, выполня-
ются грубо, поэтому не исключено, что применение $\delta^2$-процесса сходимости
замедлит итерационный процесс. Картина итераций также осложняется нали-
чием погрешности округлений, так что описанная выше схема требует практи-
ческой отработки на большом числе примеров с целью выбора оптимальных
$\eta', \eta$ и указания нижней границы значений $\varepsilon$, при которых алгоритм применим. Если однородный итерационный процесс подвергается перестройке (в нашем случае при переходе от $\textbf{x}^n$ к $\textbf{y}^n$ ), то иногда полезно проверить, не ведет ли эта перестройка к ухудшению. В качестве критерия целесообразности перестройки можно взять некоторое соотношение, связывающее нормы невязок для $\textbf{x}^n, \textbf{y}^n$ , например неравенство вида
\[
\|(E - B)\textbf{y}^n - \textbf{c}\| \leqslant q \|(E - B)\textbf{x}^n - \textbf{c} \|.
\]

Замечание о необходимости указания нижней грани значений $\varepsilon$ вызывается
следующим обстоятельством. Пусть для определенности $\lambda_{1} > 0$. Уже при вычислении $\textbf{x}^n$ по заданному $\textbf{x}^{n-1}$ погрешности округления могут возмутить ре- n
зультат на величину $\delta\textbf{x}^n$ нормой порядка $\rho$. Следствием этого может явиться возмущение $\delta\textbf{v}^n$, имеющее норму порядка $(1 − \lambda_{1})^{−1}\rho$. Отсюда следует, что в случае $\varepsilon < (1 − \lambda_{1})^{-1}\rho$ итерационный процесс может никогда не закончиться. Проведенные построения показывают, что при реализации метода возника- ет много таких моментов, разбор которых требует серьезной математической подготовки и проведения большой серии численных экспериментов. Поэтому, несмотря на «простоту» метода простой итерации, будет вполне оправданным создание стандартной программы этого метода.

\\ \\

\begin{center}
    \section*{\textsection6. Оптимизация скорости сходимости
итерационных процессов
}
\end{center}

Рассмотрим простейший итерационный способ решения системы уравне-
ний $A\textbf{x} = \textbf{b}$:
\[
x^{n-1} = \textbf{x}^{n} - \alpha(A\textbf{x}^n - \textbf{b}).
\]
\indent
Мы видели, что скорость сходимости такого итерационного процес-
са существенно зависит от максимального модуля собственных значений
матрицы $B = E − \alphaA.$ Если $\lambda_{1}, ..., \lambda_{n}$ — собственные значения матрицы $A$,
то $\max\limits_{i}\left|\lambda_{1}B\right| = \max\limits_{i}\left|1-\alpha\lambda_{1}\right|$ Из рис. 6.6.1 видно, что при действительных собственных значениях различных знаков этот максимум больше 1 и итерационный процесс расходится.
\\
\indent
Обратимся к часто встречающемуся случаю, когда все $\lambda_{i} > 0$. Значения $\lambda_{i}$ бывают известны крайне редко, однако довольно типичен случай, когда известна оценка для этих чисел вида $0< \mu \leqslant \lambda_{i} \leqslant M < \infty $ при всех $i$. Скорость сходимости итерационного процесса можно характеризовать величиной
\[
\rho(\alpha) = \max\limits_{\mu \leqslant \lambda \leqslant M} \left|1-\alpha\lambda\right|.
\]
Рассмотрим задачу минимизации $\rho(\alpha)$ за счет выбора $\alpha$.

\begin{figure}[h]

\centering

\includegraphics[width=1\linewidth]{something 2023-11-09 в 23.04.26.png}
\label{fig:mpr}

\end{figure} 
Для нахождения $\min\limits_{\alpha} \rho(\alpha)$ удобно обратиться к геометрической картине $\alpha$
(рис. 6.6.2). Ясно, что $\rho(\alpha)\geqslant 1$ при $\alpha \leqslant 0$. При $0< \alpha \leqslant M^{−1}$ функция $1−\alpha\lambda$ неотрицательна и монотонно убывает на отрезке $\left[\mu, M\right]$, поэтому $\rho(\alpha) = 1 − \alpha\mu$. При $M^{−1}  < \alpha $ величина $1 − \alpha M$ отрицательна и модуль ее растет с ростом $\alpha$. При некотором $\alpha = \alpha_{0}$ наступит момент, когда
\[
1 - \alpha_{0}\mu = -(1-\alpha_{0}M),
\eqno{(1)}
\]
и тогда $\rho(\alpha_{0}) = \left|1 - \alpha_{0}\mu\right|$ . Если $\alpha < \alpha_{0}$, то $\rho(\alpha) = 1 - \alpha\mu  > 1 - \alpha_{0}\mu = \rho(\alpha_{0})$; если $\alpha_{0} < \alpha$, то $\rho(\alpha) \geqslant \left|1-\alpha M\right| = M\alpha - 1 \geqslant M\alpha_{0} - 1 = \rho(\alpha_{0}).$

Таким образом, значение $\alpha = \alpha_{0}$ является искомым. Решая уравнение (1)
относительно $\alpha_{0}$, получим $\alpha_{0} = 2/(M + \mu).$ Отсюда
\[
\rho(\alpha_{0}) =(M - \mu) / (M + \mu).
\]
\chapter{\textbf{Задача 1. }} Доказать сходимость итерационного процесса при $\alpha = \|A\|^{-1}.$
\\
\indent
На примере систем с матрицей $A > 0$ (здесь и далее неравенство $A > 0$ означает, что $A$ — симметричная положительно определенная мат- рица) рассмотрим более формализованные постановки проблем оптимиза- ции скорости сходимости итерационных процессов.
\\
\indent
Если число ненулевых элементов матрицы много больше ее размерно- сти, то операция умножения матрицы на вектор более трудоемка, чем умножение числа на вектор или сложение векторов. Поэтому при оценке трудоемкости итерационных процессов и оптимизации этих процессов да- лее за меру трудоемкости мы неявно принимаем число умножений мат- рицы $A$ на вектор.
\\
\indent
Всякая система $A\textbf{x} = \textbf{b}$ с $det A \neq 0$, вообще говоря, может быть приве- дена (как говорят, \textit{симметризована}) умножением обеих частей уравнения на матрицу $A^T$ к системе с симметричной положительно определенной матрицей. В самом деле, система $A^T A\textbf{x} = A^T\textbf{b}$ эквивалентна исходной, матрица $A^TA$ симметричная, так как $A^TA = (A^TA)^T$, и положительно определена, так как $(A^TA\textbf{x}, \textbf{x}) = \|A\textbf{x}\|^2 > 0$ при $\textbf{x}\neq 0$. По возможности стараются избегать симметризации, поскольку, как мы увидем далее, она часто приводит к ухудшению сходимости итерационных процессов.
\\ \indent
Рассмотрим несколько более общий итерационный метод, чем метод простой итерации. А именно, в методе простой итерации

\[
\textbf{x}^{k+1} = \textbf{x}^k - \tau(A\textbf{x}^k-\textbf{b})
\]
будем считать, что итерационный параметр $\tau$ может изменяться от шага
к шагу. Тогда метод примет вид
\[
\textbf{x}^{k+1} = \textbf{x}^k - \tau_{k+1}(A\textbf{x}^k-\textbf{b}), \; \; \; k = 0, 1, ..., 
\eqno{(2)}
\]

где $\textbf{x}_{0}$—некоторое начальное приближение.
\\
\indent 
Зададимся некоторым целым $n > 0$ и произведем $n$ итераций по фор-
муле (2). Согласно (2) погрешность 
$\textbf{r}^k = \textbf{x}^k - \textbf{X}$
удовлетворяет соотно-
шению

\[\textbf{r}^{k+1} = \textbf{r}^{k}-\tau_{k+1}A\textbf{r}^{k} = (E - \tau_{k+1}A)\textbf{r}^k. \eqno{(3)} \]

Тогда через $n$ шагов итерационного метода (2) погрешность $\textbf{r}^n$ будет выражаться через погрешность начального приближения $\textbf{r}^0$ следующим
образом:
\[
\textbf{r}^n = (E - \tau_{n}A)\textbf{r}^{n-1} = ... = (E - \tau_{n}A)...(E-\tau_{1}A)\textbf{r}^0, \eqno{(4)}
\]
где $\textbf{r}_{0} = \textbf{x}_{0} - \textbf{X} - $погрешность начального приближения.

 \end{document}
